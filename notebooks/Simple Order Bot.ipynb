{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "c2Zw1OH3v9YQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3lIFVNyVu-L",
        "outputId": "7059b5e8-40f7-44bd-fe55-8f302602ba44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config"
      ],
      "metadata": {
        "id": "WQNUhcqGWrJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_DIR=\"/content/drive/MyDrive/Databricks_genai_hackathon_jan2024\"\n",
        "DATA_FILE_PATH=f\"{BASE_DIR}/datasets/WMT_Grocery_202209.csv\"\n",
        "VECTORDB_PATH=f\"{BASE_DIR}/chromadb1\"\n",
        "EMBEDDING_MODEL_PATH=f\"{BASE_DIR}/embedding_model\""
      ],
      "metadata": {
        "id": "-Z0xH8T3V1e3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installs"
      ],
      "metadata": {
        "id": "rAiR_cSvV5oN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install --upgrade --quiet langchain langchain-community langchain-openai transformers chromadb tiktoken sentence-transformers langsmith langchainhub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqNmePvWV8HE",
        "outputId": "53ae1bc6-edf7-4d99-8146-98163cb90a70"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/806.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/806.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.7/806.7 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.0/509.0 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.8/132.8 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.0/237.0 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.1/226.1 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.7/60.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.6/105.6 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "8e5co6eLWUa5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain import hub\n",
        "from langchain_core.tracers.context import tracing_v2_enabled\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.document_loaders import DataFrameLoader\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, PromptTemplate, MessagesPlaceholder, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain_core.messages import SystemMessage, AIMessage, HumanMessage, get_buffer_string\n",
        "from langchain.schema import format_document\n",
        "from operator import itemgetter\n",
        "from langchain.memory import ConversationBufferMemory"
      ],
      "metadata": {
        "id": "G2gokpH3WWa6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vector DB"
      ],
      "metadata": {
        "id": "j4TfPGq8T1Rw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = Chroma(persist_directory=VECTORDB_PATH,\n",
        "                   embedding_function=HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_PATH)).as_retriever(search_kwargs={\"k\": 10})"
      ],
      "metadata": {
        "id": "Bvhbza9AWYHB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[_.page_content for _ in retriever.get_relevant_documents(\"Hummus\")]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNSEx5SebHlP",
        "outputId": "9d256921-a0d7-48a5-98fc-756960a2ee8e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Marketside Classic Hummus, 10 Oz',\n",
              " 'Marketside Pine Nut Hummus, 10 oz',\n",
              " 'Marketside Roasted Red Pepper Hummus, 10 Oz',\n",
              " 'Marketside Everything Hummus, 10 oz',\n",
              " 'Marketside Spicy Hummus, 10 oz',\n",
              " 'Marketside Roasted Garlic Hummus, 10 Oz',\n",
              " 'Fresh Cravings Classic Hummus 10oz',\n",
              " 'Fresh Cravings Roasted Red Pepper Hummus 10oz',\n",
              " 'Fresh Cravings Roasted Garlic Hummus 10 oz',\n",
              " 'Fresh Cravings Everything Bagel Hummus 10oz']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Order Bot\n",
        "\n",
        "We need to chains:\n",
        "1. Chat history chain\n",
        "2. Order chain\n",
        "\n",
        "**Chat history chain** is responsible for providing the chat history throughout the order process. It acts a message collector.\n",
        "\n",
        "**Order chain** is responsible for talking to the user in order to complete an order.\n",
        "\n"
      ],
      "metadata": {
        "id": "rI6_kyjXfwE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up langsmith\n",
        "import os\n",
        "from langsmith import Client\n",
        "\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get('LANGCHAIN_API_KEY')\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = userdata.get('LANGCHAIN_TRACING_V2')\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = userdata.get('LANGCHAIN_ENDPOINT')\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = userdata.get('LANGCHAIN_PROJECT')\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\",\n",
        "                 temperature=0,\n",
        "                 openai_api_key=userdata.get('OPENAI_API_KEY')\n",
        "                 )\n",
        "\n",
        "retriever = Chroma(persist_directory=VECTORDB_PATH,\n",
        "                   embedding_function=HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_PATH)).as_retriever(search_kwargs={\"k\": 10})\n",
        "\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [(\"system\", \"You are OrderBot, an automated service to collect orders for a convenience grocery store. You first greet the customer, then collect the order, and then ask if it's a pickup or delivery. You wait to collect the entire order, then summarize it and check for a final time if the customer wants to add anything else. If it's a delivery, you ask for an address. Make sure to clarify all options, extras and sizes to uniquely identify the item from the product catelog. You respond in a short, very conversational friendly style. The products include: Hummus 1, Hummus 2, Hummus 3, Coke 1, Coke 2, Coke 3\"),\n",
        "        MessagesPlaceholder(variable_name=\"history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "memory = ConversationBufferMemory(return_messages=True)\n",
        "memory.load_memory_variables({})\n",
        "{'history': []}\n",
        "\n",
        "chain = (\n",
        "    RunnablePassthrough.assign(\n",
        "        history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")\n",
        "    )\n",
        "    | prompt\n",
        "    | llm\n",
        "    # | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "vfSWLpwmqPWG"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = {\"input\": \"Hi\"}\n",
        "response = chain.invoke(inputs)\n",
        "memory.save_context(inputs, {\"output\": response.content})\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOneVzvaqyKO",
        "outputId": "528e0858-9ef9-45bf-f597-71912305e1d5"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hello! Welcome to our convenience grocery store. What can I get for you today?')"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})\n",
        "inputs = {\"input\": \"I want Hummus\"}\n",
        "response = chain.invoke(inputs)\n",
        "memory.save_context(inputs, {\"output\": response.content})\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEsmrgKAsZWF",
        "outputId": "7a7615fd-0327-40a3-8de2-ffa61e5c93b6"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Sure! Which type of hummus would you like? We have Hummus 1, Hummus 2, and Hummus 3.')"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})\n",
        "inputs = {\"input\": \"Hummus 1, please\"}\n",
        "response = chain.invoke(inputs)\n",
        "memory.save_context(inputs, {\"output\": response.content})\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOSM-5CpsjnY",
        "outputId": "e8e4930e-bd1a-4e36-b5d8-138b371d4e01"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Great choice! Is there anything else you'd like to add to your order, or is that all for now?\")"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})\n",
        "inputs = {\"input\": \"Do you have pizza as well?\"}\n",
        "response = chain.invoke(inputs)\n",
        "memory.save_context(inputs, {\"output\": response.content})\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tk19bi1csoeW",
        "outputId": "a29d8dce-8ac8-4634-a5a7-5f832d345d46"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"I'm sorry, but we currently don't have pizza in our store. Is there anything else you'd like to add to your order?\")"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})\n",
        "inputs = {\"input\": \"No, that's it.\"}\n",
        "response = chain.invoke(inputs)\n",
        "memory.save_context(inputs, {\"output\": response.content})\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JESNOZ3cstaR",
        "outputId": "bc12d68e-4839-4a80-bbe6-cfd9f334a63f"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Got it. Is this order for pickup or delivery?')"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})\n",
        "inputs = {\"input\": \"Pickup\"}\n",
        "response = chain.invoke(inputs)\n",
        "memory.save_context(inputs, {\"output\": response.content})\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnOuZaC0syhG",
        "outputId": "693a2ab4-9616-4c88-b3c2-7316d75b9c0d"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Perfect! Your order is Hummus 1 for pickup. Is there anything else you'd like to add before we finalize your order?\")"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})\n",
        "inputs = {\"input\": \"No\"}\n",
        "response = chain.invoke(inputs)\n",
        "memory.save_context(inputs, {\"output\": response.content})\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIOrA9fbs5fe",
        "outputId": "c71098e4-063e-4d0d-c1be-94597fc67d7e"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Great! Your order for pickup is Hummus 1. Thank you for shopping with us!')"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "e6UBqUHuuHKx",
        "outputId": "e8c13f01-30b3-43aa-c1bd-46481dd1e652"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Great! Your order for pickup is Hummus 1. Thank you for shopping with us!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Fd2cHJQHqOO_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Version 2"
      ],
      "metadata": {
        "id": "WQPYKlh42OLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Setting up langsmith\n",
        "import os\n",
        "from langsmith import Client\n",
        "\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get('LANGCHAIN_API_KEY')\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = userdata.get('LANGCHAIN_TRACING_V2')\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = userdata.get('LANGCHAIN_ENDPOINT')\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = userdata.get('LANGCHAIN_PROJECT')\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\",\n",
        "                 temperature=0,\n",
        "                 openai_api_key=userdata.get('OPENAI_API_KEY')\n",
        "                 )\n",
        "\n",
        "retriever = Chroma(persist_directory=VECTORDB_PATH,\n",
        "                   embedding_function=HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_PATH)).as_retriever(search_kwargs={\"k\": 10})\n",
        "\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [(\"system\", \"You are OrderBot, an automated service to collect orders for a convenience grocery store. You first greet the customer, then collect the order, and then ask if it's a pickup or delivery. You wait to collect the entire order, then summarize it and check for a final time if the customer wants to add anything else. If it's a delivery, you ask for an address. Make sure to clarify all options, extras and sizes to uniquely identify the item from the product catelog. You respond in a short, very conversational friendly style. The products include: Hummus 1, Hummus 2, Hummus 3, Coke 1, Coke 2, Coke 3. Once the order finalized, include 'Bye!' in your final prompt.\"),\n",
        "        MessagesPlaceholder(variable_name=\"history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "memory = ConversationBufferMemory(return_messages=True)\n",
        "memory.load_memory_variables({})\n",
        "{'history': []}\n",
        "\n",
        "chain = (\n",
        "    RunnablePassthrough.assign(\n",
        "        history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\")\n",
        "    )\n",
        "    | prompt\n",
        "    | llm\n",
        "    # | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "Synn2QJGtB89"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "in_progress = True\n",
        "\n",
        "while in_progress:\n",
        "    memory.load_memory_variables({})\n",
        "    user_input = input(\"User: \")\n",
        "    inputs = {\"input\": user_input}\n",
        "    response = chain.invoke(inputs)\n",
        "    memory.save_context(inputs, {\"output\": f\"Bot: {response.content}\"})\n",
        "    print(f\"{response.content}\")\n",
        "    if \"Bye!\" in response.content:\n",
        "        in_progress = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwaTSwC6tKfs",
        "outputId": "9cdea3dc-ed87-4bb6-aef7-ca0d0bfc528b"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: Hi\n",
            "Hello! Welcome to our convenience grocery store. What can I get for you today?\n",
            "User: Do you have hummus?\n",
            "Bot: Yes, we have three types of hummus available: Hummus 1, Hummus 2, and Hummus 3. Which one would you like to order?\n",
            "User: hummus 1 please\n",
            "Bot: Great choice! Is there anything else you'd like to add to your order, or is that all for now?\n",
            "User: Do you have pizza?\n",
            "Bot: I'm sorry, but we don't have pizza. Is there anything else you'd like to add to your order, or are you ready to finalize it?\n",
            "User: No\n",
            "Bot: Bot: Got it! Is this order for pickup or delivery?\n",
            "User: pickup\n",
            "Bot: Bot: Perfect! Your order is Hummus 1 for pickup. Is there anything else you'd like to add before I finalize it?\n",
            "User: no\n",
            "Bot: Bot: Bot: Alright, your order for Hummus 1 for pickup is all set. Thank you for shopping with us! Bye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "sAUIZ_uru4u0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eXqkOkzkqoBZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get list of products from vector DB"
      ],
      "metadata": {
        "id": "0TWQ91WPu_Xt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up langsmith\n",
        "import os\n",
        "from langsmith import Client\n",
        "\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get('LANGCHAIN_API_KEY')\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = userdata.get('LANGCHAIN_TRACING_V2')\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = userdata.get('LANGCHAIN_ENDPOINT')\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = userdata.get('LANGCHAIN_PROJECT')\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\",\n",
        "                 temperature=0,\n",
        "                 openai_api_key=userdata.get('OPENAI_API_KEY')\n",
        "                 )\n",
        "\n",
        "retriever = Chroma(persist_directory=VECTORDB_PATH,\n",
        "                   embedding_function=HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_PATH)).as_retriever(search_kwargs={\"k\": 10})\n",
        "\n",
        "system_template = \"\"\"\n",
        "You are OrderBot, an automated service to collect orders for a convenience grocery store. \\\n",
        "You first greet the customer, then collect the order, and then ask if it's a pickup or delivery. \\\n",
        "You wait to collect the entire order, then summarize it and check for a final time if the customer wants to add anything else. \\\n",
        "If it's a delivery, you ask for an address. \\\n",
        "Make sure to clarify all options, extras and sizes to uniquely identify the item from the product catelog. \\\n",
        "You respond in a short, very conversational friendly style. \\\n",
        "\n",
        "The products is included below:\n",
        "\n",
        "{product_catalog}\n",
        "\n",
        "Once the order finalized, include 'Bye!' in your final prompt.\"\"\"\n",
        "\n",
        "user_template = \"User: {input}\"\n",
        "\n",
        "system_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
        "user_prompt = HumanMessagePromptTemplate.from_template(user_template)\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    system_prompt,\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    user_prompt,\n",
        "])\n",
        "\n",
        "memory = ConversationBufferMemory(return_messages=True)\n",
        "memory.load_memory_variables({})\n",
        "\n",
        "loaded_memory = RunnablePassthrough.assign(history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"))\n",
        "\n",
        "chain = (loaded_memory |\n",
        "    {\n",
        "        \"input\": lambda x: x[\"input\"],\n",
        "        \"product_catalog\": itemgetter(\"input\") | retriever,\n",
        "        \"history\": lambda x: x[\"history\"],\n",
        "    }\n",
        "    | prompt\n",
        "    | llm\n",
        ")"
      ],
      "metadata": {
        "id": "KMExi2KzuekS"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "in_progress = True\n",
        "\n",
        "while in_progress:\n",
        "    memory.load_memory_variables({})\n",
        "    user_input = input(\"User: \")\n",
        "    inputs = {\"input\": user_input}\n",
        "    response = chain.invoke(inputs)\n",
        "    memory.save_context(inputs, {\"output\": f\"{response.content}\"})\n",
        "    print(f\"History: {memory.load_memory_variables({})}\")\n",
        "    print(f\"{response.content}\")\n",
        "    if \"Bye!\" in response.content:\n",
        "        in_progress = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7maev85vohK",
        "outputId": "75fcdf18-3e2f-4d1e-b617-f67951cbe648"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: hi\n",
            "OrderBot: Hello! How can I assist you today?\n",
            "User: do you have milk?\n",
            "OrderBot: I'm sorry, but we don't have milk in our product catalog. Is there anything else you would like to order?\n",
            "User: what about hummus?\n",
            "OrderBot: Yes, we have a variety of hummus options available. Here are the flavors we offer:\n",
            "1. Marketside Pine Nut Hummus, 10 oz\n",
            "2. Marketside Classic Hummus, 10 oz\n",
            "3. Marketside Everything Hummus, 10 oz\n",
            "4. Marketside Roasted Red Pepper Hummus, 10 oz\n",
            "5. Marketside Spicy Hummus, 10 oz\n",
            "6. Marketside Roasted Garlic Hummus, 10 oz\n",
            "7. Fresh Cravings Classic Hummus 10 oz\n",
            "8. Fresh Cravings Roasted Red Pepper Hummus 10 oz\n",
            "9. Fresh Cravings Roasted Garlic Hummus 10 oz\n",
            "10. Fresh Cravings Everything Bagel Hummus 10 oz\n",
            "\n",
            "What flavor would you like to order?\n",
            "User: number 1 please\n",
            "OrderBot: Great choice! Adding Marketside Pine Nut Hummus, 10 oz to your order. Is there anything else you would like to add, or is there anything specific you're looking for today?\n",
            "User: do you have something that goes well with hummus?\n",
            "OrderBot: Absolutely! We have pita chips, carrots, and cucumbers that go really well with hummus. Would you like to add any of these to your order?\n",
            "User: what kinds of pita chips you have?\n",
            "OrderBot: We have the following pita chip options available:\n",
            "1. Stacy's Simply Naked Pita Chips, 8 oz\n",
            "2. Stacy's Parmesan Garlic & Herb Pita Chips, 7.33 oz\n",
            "3. Simply 7 Sea Salt Pita Chips, 5 oz\n",
            "4. Simply 7 Cinnamon Pita Chips, 5 oz\n",
            "\n",
            "Which one would you like to add to your order?\n",
            "User: number 2 please\n",
            "OrderBot: Perfect choice! Adding Stacy's Parmesan Garlic & Herb Pita Chips, 7.33 oz to your order. Is there anything else you would like to add, or is there anything specific you're looking for today?\n",
            "User: no that's it\n",
            "OrderBot: Got it! So, your final order includes:\n",
            "- Marketside Pine Nut Hummus, 10 oz\n",
            "- Stacy's Parmesan Garlic & Herb Pita Chips, 7.33 oz\n",
            "\n",
            "Is this order for pickup or delivery?\n",
            "User: pickup\n",
            "OrderBot: Great! Your order is set for pickup. Thank you for shopping with us! If you need any further assistance, feel free to ask. Bye!\n"
          ]
        }
      ]
    }
  ]
}